{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPN34UUZBGuhSySQAUYe1oa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshita27-cmd/Warehouse-Robot-Policy-Based-Models/blob/main/warehouse_robot_path_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy matplotlib # PyTorch is used in neural networks and reinforcement learning. numpy is used for multi dimensional arrays. matplotlib is used for visualization"
      ],
      "metadata": {
        "id": "yzwR4niRnJSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab80cf99-05b8-4ba9-b1c4-bbedcde83929",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Installing collected packages: nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each cell is:\n",
        "\n",
        "游릴 = empty\n",
        "\n",
        "游닍 = item location (pickup)\n",
        "\n",
        "游꿢 = drop location\n",
        "\n",
        "游뱄 = robot"
      ],
      "metadata": {
        "id": "Mcdsx72RoWx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# building a simple warehouse environment (grid world)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "# warehouse grid environment\n",
        "class WarehouseEnv:\n",
        "  def __init__(self,size=5): # size of the grid is 5\n",
        "    self.size=size\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.grid=np.zeros((self.size,self.size)) # 2D array of 5X5 size filled with 0's that shows empty spaces\n",
        "    self.robot_pos=[0,0] #top left corner[0,0] is the position of the robot initialy\n",
        "    self.pickup_pos=[self.size-1,0] #pickup position for the robot where it will find the product\n",
        "    self.dropoff_pos=[0,self.size-1] # drop off position for a item at top right corner [0,4]\n",
        "    self.has_item=False #robot currently dosen't have any product\n",
        "    self.steps=0 # number of actions or the track of actions  taken by the robot\n",
        "    return self._get_state()\n",
        "\n",
        "  def _get_state(self):\n",
        "    return np.array(self.robot_pos + [int(self.has_item)]) # robot position was on the grid with x,y position. and has_item is a boolean value to integer(0=false,1). now ehat returned is [0,0,1] 0,0  will be the position and 1 is that the robot is carrying something\n",
        "\n",
        "  def  _is_valid(self, pos):\n",
        "    return 0 <=pos[0] < self.size and 0 <= pos[1] <self.size # checking if the position ofthe robot is within the valid grid. pos[0] checks if x coordinate is withing the valid range and similarly pos[1] checks if y is in a valid range. and it return boolean value(true, false)\n",
        "\n",
        "  def step(self,action): # 0 = up, 1=down, 2=left, 3=right\n",
        "    move =[[-1,0],[1,0],[0,-1],[0,1]] # [-1,0] moves up( decrease row index by 1), [1,0] move down, [0,-1] move left, [0,1] move right\n",
        "    next_pos=[self.robot_pos[0] + move[action][0], self.robot_pos[1] + move[action][1]] # robot_pos[0] is for x axis and move[action][0] with it is for action at x axis and y remains same. example: if the robot position is [2,2] and the action is 0 (up) then next_pos= [2+[-1,0]],[2+[0,0]] which give [1,2]\n",
        "    reward=-0.1 # penalty for each movement so that robot can find solution eary\n",
        "\n",
        "    if self._is_valid(next_pos):\n",
        "      self.robot_pos=next_pos\n",
        "\n",
        "    # pickup logic\n",
        "    if self.robot_pos==self.pickup_pos and not self.has_item: # self.robot_pos==self.pickup_pos checks if the robot is at the pickup position. not self.has_item checks if the robot disen't have the item\n",
        "      self.has_item=True # if true the robot pickups item and sets flag indicating it's carrying something\n",
        "      reward= +1.0 # reward for picking up the item\n",
        "\n",
        "    # drop off logic\n",
        "    elif self.robot_pos==self.dropoff_pos and self.has_item:\n",
        "      self.has_item=False\n",
        "      reward=+2.0\n",
        "\n",
        "    self.steps+=1 # calculating how many stepsagent took\n",
        "    done=self.steps > 50  # cheks if the agent has exceeded more than 50 if True the episode is maked as done.steps to avoid invinite loop\n",
        "    return self._get_state(),reward, done # self._get_state() is the new state of the environment after the action\n",
        "\n",
        "  # visual representation of the robot\n",
        "  def render(self):\n",
        "    grid=np.full((self.size,self.size),\"拘\")\n",
        "    x,y=self.robot_pos\n",
        "    grid[x][y]='游뱄'\n",
        "    grid[self.pickup_pos[0]][self.pickup_pos[1]]='游닍'\n",
        "    grid[self.dropoff_pos[0]][self.dropoff_pos[1]]='游꿢'\n",
        "    for row in grid:\n",
        "      print(''.join(row))\n",
        "    print()"
      ],
      "metadata": {
        "id": "1gJ7-g52ndh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env=WarehouseEnv()\n",
        "state=env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STswWOxSYCje",
        "outputId": "2a5de12f-7d79-4a98-ae5b-3dcd93b80701",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "游뱄拘럭샢拘럻릝슢n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for agent\n",
        "\n",
        "import torch # torchhas similar libraries as numpy but it can also use GPU fir accelerated computing, automatic difference is needed in neural network and backprogation\n",
        "import torch.nn as nn # for neural network layers and loss functions\n",
        "import torch.optim as optim # consists of optimization algorithm that are used to update the parameters of neural networks during training. it has SGD(stochastic Gradient Descent) which updates parameters based on gradient of loss function. it also include adam which is an adaptive learning rate that adjusts learning rate for each parameter based on first and second moments of gradient. these optimizers are used to minimize loss functions\n",
        "class PolicyNetwork(nn.Module):\n",
        "  def __init__(self, input_dim,hidden_dim,output_dim): # input_dim is size of input(state representation), hidden_dim is number if neurons in hidden layers, output_dim is the no. of possible actions\n",
        "    super(PolicyNetwork,self).__init__()\n",
        "    self.fc1=nn.Linear(input_dim,hidden_dim) # first fully connected linear layer( input_dimention -> hidden dimention)\n",
        "    self.relu=nn.ReLU() # relu is activation function for non-linearity\n",
        "    self.fc2=nn.Linear(hidden_dim,output_dim) # second fully connected layer (hidden_dim -> output_dim)\n",
        "    self.softmax=nn.Softmax(dim=-1) # softmax converts action score into probabilities (summing to 1). example: input=[2,3,0] where 2,3is the positiona nd 0 means no item. output=[0.1,0.6,0.2,0.1] 60% chance to move down\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.relu(self.fc1(x)) # input x gets the current state( example: robot position+ environment info). fc1 is the linear transformation (input_dim -> hidden_dim) applying relu.\n",
        "    y=self.softmax(self.fc2(x)) # linear transformation (hiiden_dim -> output_dim (no. of actions)). softmax converts to probabilities\n",
        "    return y\n"
      ],
      "metadata": {
        "id": "mubm-H4_apq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training reinforce\n",
        "policy=PolicyNetwork(input_dim=3, hidden_dim=128, output_dim=4) # input_dim=3 means there are three inputs (x_pos,y_pos,had_item). output_dim=4 means 4 possible actions(up,down,right,left)\n",
        "optimizer=optim.Adam(policy.parameters(),lr=0.01) # lr is learning rate. ot how drastically weights update each step. used adam optimizer to update the network weights. actumatically tracks all trainable parameters in policy network\n",
        "gamma=0.99 # discount for future rewards\n",
        "def select_action(state): # state is taken as an imput. its the current state\n",
        "  state=torch.FloatTensor(state) # converting the 'state' to PyTorch tensor of FloatTensor as PyTorch requires inputs to be in tensor format\n",
        "  probs=policy(state) # getting the output probabilities of each action\n",
        "  dist=torch.distributions.Categorical(probs) # the probabilities got from above can be used to make samples or log probabiloties. the log probabilities and samples comes under Categorial\n",
        "  action=dist.sample() # randomly select the any action but still the chances of selecting the highest probability is much more\n",
        "  return action.item(),dist.log_prob(action)\n",
        "\n",
        "def compute_returns(rewards,gamma): # rewards are a list or an array of rewarrdsreceived at each step during an episode. and gamma is high means future rewards are considered more important\n",
        "  returns=[] # to store the returns of each time step\n",
        "  G=0 # G will accumulate the discount returns as we integrate through the rewards. discount return is sum of all the rewards from time step onwards discounted by the factor.\n",
        "  for r in reversed(rewards): # reverse the list because returns at each step depends on current reward and the returns of future time steps\n",
        "    G=r+gamma*G # for r G (discount return is updated) including gamma\n",
        "    returns.insert(0,G) # as earlier we computed by reversing now we need to get the correct orders for rewards.   this line ensures the G is int]serted at the 0 th or the initial position to get the correct order\n",
        "  return returns\n",
        "\n"
      ],
      "metadata": {
        "id": "E11MFtZFhpU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the agent\n",
        "num_episodes=500\n",
        "for episodes in range (num_episodes):\n",
        "  state=env.reset()\n",
        "  log_probs=[]\n",
        "  rewards=[]\n",
        "  total_reward=0\n",
        "\n",
        "  for t in range(100):\n",
        "    action,log_prob=select_action(state) # determine which action should be choosed\n",
        "    next_state,reward,done=env.step(action)\n",
        "    log_probs.append(log_prob)\n",
        "    rewards.append(reward)\n",
        "    state=next_state\n",
        "    total_reward += reward\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  returns=compute_returns(rewards,gamma) # calculating the discounted cummilative sum of rewards which is total return for each time step\n",
        "  returns=torch.tensor(returns) # returns are converted into PyTorch tensor for numerical computations\n",
        "  returns=(returns-returns.mean())/(returns.std()+ 1e-9) # normalising the rewards, normalizing can help reducing variable variance, preventing Exploding Gradients\n",
        "  loss=0\n",
        "  for log_prob,G in zip(log_probs,returns):\n",
        "    loss -= log_prob * G # in the policy formulab its taken positive but since in PyTorch we need to minimize loss therefore the formula is taken negative. it works as if G is high(good reward), gradient will increase the probability of that action and if G is ow or negative gradient will decrease probability of that action\n",
        "\n",
        "  optimizer.zero_grad() # PyTorch accumulates gradients by defaults and we want ti start fresh for each iteration therefore we zero the gradients\n",
        "  loss.backward() # computing gradient of loss with respect to models parameters. this is done using backprpogation\n",
        "  optimizer.step() # to update the parameters based on gradients in previous step. it adjusts parameters to minimize loss\n",
        "# first we zeroed the gradients to make sure we are not accumulating gradients from the previous iteration. we then calculate the gradient of loss with respect to models parameters using backpropagation that gives us the direction in which we need to adjust models parameters to minimize the loss. finally, parameters are updated using gradients computed in the previous step\n",
        "  if episodes % 50 == 0: # to print the episodes if they episode is a multiple of 50\n",
        "    print(f'episode {episodes}, Total reward: {total_reward:.2f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khj1Rnc3ctNB",
        "outputId": "bc35bd7f-ee11-4456-a667-0da5a44f6db0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode 0, Total reward: -4.00\n",
            "episode 50, Total reward: -4.00\n",
            "episode 100, Total reward: -4.00\n",
            "episode 150, Total reward: -4.00\n",
            "episode 200, Total reward: -4.00\n",
            "episode 250, Total reward: -4.00\n",
            "episode 300, Total reward: -4.00\n",
            "episode 350, Total reward: -4.00\n",
            "episode 400, Total reward: -4.00\n",
            "episode 450, Total reward: -4.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing and visualize\n",
        "state=env.reset()\n",
        "env.render()\n",
        "for _ in range(20):\n",
        "  action,_=select_action(state)\n",
        "  state,_,done=env.step(action)\n",
        "  env.render()\n",
        "  if done:\n",
        "    break"
      ],
      "metadata": {
        "id": "GKAXdGcpVQXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c862d58a-c134-44a0-bad8-725007083916",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "游뱄拘럭샢拘럻릝슢n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "游뱄拘럭샢拘럭샢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "游뱄拘럭샢拘럭샢\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游뱄拘럭샢拘럭샢\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n",
            "拘럭샢拘럭샢游꿢\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "拘럭샢拘럭샢拘\n",
            "游닍拘럭샢拘럭샢\n",
            "\n"
          ]
        }
      ]
    }
  ]
}